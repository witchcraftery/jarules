# JaRules - Your Async Development Agent ðŸ¤–

![JaRules Asynchronous Development Assistant](https://raw.githubusercontent.com/witchcraftery/jarules/main/jarules_title-card_b.png)

## Overview

JaRules is an asynchronous development assistant designed to streamline your software engineering tasks. It connects to your local file system, GitHub repositories, and various AI APIs. Through an intuitive interface (initially CLI, evolving to a chat-based UI), JaRules aims to help you write code, tackle bugs, manage features, and more!

JaRules now supports multiple LLM backends (including Ollama for local models, OpenRouter for diverse cloud models, and Anthropic Claude alongside Google Gemini) and allows you to switch between them seamlessly at runtime via the CLI.

## Current Status

ðŸŽ‰ **MAJOR MILESTONE ACHIEVED: All Tests Passing!** ðŸŽ‰

âœ… **Test Suite Complete**: 40/40 Gemini API tests passing  
âœ… **LLM Architecture Complete**: Multi-provider foundation ready  
âœ… **Clean CI/CD**: Full regression protection for AI features  

## Recent Development Status (May 2025)

### ðŸŽ‰ **MAJOR MILESTONE: Test Suite Validation Complete**

**Status**: **ALL TESTS PASSING - 100% SUCCESS** âœ…

We've successfully resolved all test suite issues and achieved complete test coverage:

- **40/40 Gemini API tests passing** ðŸŽ¯
- **Environment variable patching fixed** - Core initialization issue resolved
- **Mock setup corrected** - Proper API interaction testing
- **Import references fixed** - Correct Google AI type handling
- **Error handling validated** - Comprehensive edge case coverage

**Technical Achievement**: 
- Fixed critical setUp() method timing issues across all test classes
- Resolved `genai.protos.Candidate.FinishReason` reference problems  
- Implemented safe enum handling with `_get_enum_name()` helper
- Aligned test expectations with actual client implementation

### âœ… **COMPLETED: LLM Configuration System**

- **LLMManager Architecture**: Flexible multi-LLM support system implemented
- **Configuration Management**: YAML-based LLM configuration with `config/llm_config.yaml`
- **Gemini Integration**: Fully integrated with configurable parameters
- **CLI Integration**: Updated to use LLMManager for AI operations
- **Complete Test Coverage**: All components fully tested and validated

### ðŸš€ **READY FOR: Multi-LLM Expansion**

With our solid, tested foundation in place, we're ready to expand:

- **Ollama Integration**: Local model support (CodeLlama, Llama 3, Mistral)
- **OpenRouter Connector**: Cloud model diversity access
- **Anthropic Claude**: Claude API integration
- **Runtime Model Switching**: Dynamic provider selection

### âœ… **Multi-LLM Expansion & CLI Switching Completed (June 2025)**

Building on the robust LLM configuration system, JaRules has been expanded to include support for several new LLM providers and runtime model management:

-   **Ollama Integration**: Enables connection to local LLMs managed by Ollama.
-   **OpenRouter Connector**: Provides access to a wide variety of models via the OpenRouter API.
-   **Anthropic Claude Integration**: Supports Claude models (e.g., Haiku, Sonnet, Opus) via the official Anthropic SDK.
-   **Runtime Model Switching & Persistence**: Implemented CLI commands (`set-model <id>`, `get-model`, `clear-model`) allowing users to dynamically switch the active LLM provider. User selections are persisted in `~/.jarules/user_state.json`.

## Progress So Far (What's Built! âœ¨)

### **Core Architecture**

- **Project Structure:** Solid foundation with `core`, `connectors`, `ui`, and `tests`

- **LLM Management System**: Flexible architecture supporting multiple AI providers
- **Configuration System**: YAML-based configuration for easy LLM provider management

### **Local File System (`jarules_agent/connectors/local_files.py`)**

- List files and directories (`ls <path>`)

- Read file content (`read <path>`)
- Write content to files (`write <path> <content>`)

### **GitHub Connector (`jarules_agent/connectors/github_connector.py`)**

- **Read Operations:**
  - `list_repo_files(owner, repo, path)`: Lists files and directories in a repository path
  - `read_repo_file(owner, repo, file_path)`: Reads the content of a file from a repository

- **Write Operations:**
  - `create_branch(owner, repo, new_branch_name, source_branch_name)`: Creates a new branch from a source branch
  - `commit_files(owner, repo, branch_name, file_changes, commit_message)`: Commits one or more file changes to a branch
  - `create_pull_request(owner, repo, head_branch, base_branch, title, body)`: Creates a new pull request

### **AI Integration (`jarules_agent/connectors/gemini_api.py`)**

- **LLMManager Integration**: Configurable Gemini client via LLMManager

- **Core AI Functions:**
  - `generate_code(user_prompt, system_instruction)`: Generates code based on prompts
  - `explain_code(code_snippet, system_instruction)`: Explains code snippets
  - `suggest_code_modification(code_snippet, issue_description, system_instruction)`: Suggests code improvements
- **Configuration Support**: API key management, default system prompts, generation parameters
- **Ollama Integration (`jarules_agent/connectors/ollama_connector.py`)**: Enables connection to local LLMs managed by Ollama.
- **OpenRouter Integration (`jarules_agent/connectors/openrouter_connector.py`)**: Provides access to a wide variety of models via the OpenRouter API.
- **Anthropic Claude Integration (`jarules_agent/connectors/claude_connector.py`)**: Supports Claude models (e.g., Haiku, Sonnet, Opus) via the official Anthropic SDK.

### **Command-Line Interface (`jarules_agent/ui/cli.py`)**

- **LLMManager Integration**: Uses LLMManager for AI operations

- **Available Commands:**
  - **File Operations**: `ls`, `read`, `write`
  - **GitHub Operations**: `gh_ls`, `gh_read`
  - **AI Operations**: `ai gencode`, `ai explain`, `ai explain_file`, `ai suggest_fix`, `ai suggest_fix_file`
  - **Model Management**:
    - `set-model <provider_id>`: Sets the active LLM provider.
    - `get-model`: Displays the current active LLM provider and its configuration.
    - `clear-model`: Clears any persisted active model selection and reverts to the default.
  - **Utility**: `help`, `exit`

### **Testing Suite**

- **Core Component Tests**: LLMManager, Gemini API, local files, GitHub connector

- **Integration Tests**: CLI component testing (fixes in progress)
- **Error Handling**: Comprehensive error scenario coverage

## How to Run JaRules (CLI)

1. **Clone the repository**:

    ```bash
    git clone https://github.com/witchcraftery/jarules.git
    cd jarules
    ```

2. **Set up a virtual environment (recommended)**:

    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows use `venv\Scripts\activate`
    ```

3. **Install dependencies**:

    ```bash
    pip install -r requirements.txt
    ```

4. **Set up Gemini API key** (for AI features):

    ```bash
    export GEMINI_API_KEY="your_gemini_api_key_here"
    ```

5. **Run the CLI**:

    ```bash
    python -m jarules_agent.main
    ```

    (Alternatively: `python jarules_agent/main.py`)

6. **Type `help`** in the JaRules CLI to see available commands!

## Testing

### **Run All Tests**

```bash
# Full test suite
python -m pytest jarules_agent/tests/ -v

# Specific test file
python -m pytest jarules_agent/tests/test_llm_manager.py -v
```

### **Current Test Status**

- âœ… **Gemini API tests**: All 40 tests passing (100% success rate)
- âœ… **LLMManager tests**: All passing
- âœ… **Local files tests**: All passing
- âœ… **GitHub connector tests**: All passing
- âœ… **Core architecture**: Fully tested and validated

**Testing Achievement**: Complete regression protection for AI features with bulletproof test coverage!

## Development Status & Next Steps

### Current Development Priority (Electron UI - Phase 2)

With the core multi-LLM capabilities, CLI-based model management, and the foundational Electron UI (Phase 1) now complete, the next major development thrust is **Electron UI - Phase 2: Advanced Features & Foolproofing**. Key tasks include:

1.  **In-App Configuration UI (Read-Only First)**: Display `llm_config.yaml` content in a user-friendly, read-only format within the Electron app.
2.  **"Stop Generation" Functionality**: Allow users to interrupt in-progress LLM response streams.
3.  **Pre-flight Checks / Diagnostics**: Implement a panel to verify backend components, configurations, and connectivity.

Following this, development will proceed to **Electron UI - Phase 2.5: Asynchronous Git-Split Task Completion** and then **Electron UI - Phase 3: Polish & Packaging**. Advanced agent features will be developed as a parallel track or subsequent major focus. Refer to `IMPLEMENTATION_GUIDE.md` for full details.

### **Long-term Vision**

1. **Chat User Interface**: Fully featured Electron-based desktop application.
2. **Advanced Agent Capabilities**: Robust multi-step task execution, planning, and self-correction.
3. **Plugin System**: Highly extensible connector and tool architecture.
4. **Web UI / Backend Service**: Potential for a web-accessible version.

## Contributing

We're actively developing JaRules and welcome contributions!

### **Current Contribution Opportunities**

- **Multi-LLM Development**: Help implement Ollama, OpenRouter, or Claude connectors
- **CLI Enhancement**: Improve user experience and add model switching features
- **Documentation**: Enhance guides and API documentation
- **Testing**: Expand test coverage for new LLM providers

### **Development Setup**

1. Fork and clone the repository
2. Set up development environment as described above
3. Run tests to ensure everything works (`python -m pytest jarules_agent/tests/ -v`)
4. Check `IMPLEMENTATION_GUIDE.md` for current development priorities

## Architecture

JaRules follows a modular architecture:

```
jarules_agent/
â”œâ”€â”€ core/               # Core logic and management
â”‚   â””â”€â”€ llm_manager.py  # LLM provider management
â”œâ”€â”€ connectors/         # External service integrations
â”‚   â”œâ”€â”€ local_files.py  # File system operations
â”‚   â”œâ”€â”€ github_connector.py  # GitHub API integration
â”‚   â”œâ”€â”€ gemini_api.py   # Google Gemini integration
â”‚   â”œâ”€â”€ ollama_connector.py # Ollama local LLM integration
â”‚   â”œâ”€â”€ openrouter_connector.py # OpenRouter API integration
â”‚   â”œâ”€â”€ claude_connector.py # Anthropic Claude API integration
â”‚   â””â”€â”€ base_llm_connector.py  # LLM connector interface
â”œâ”€â”€ ui/                 # User interfaces
â”‚   â””â”€â”€ cli.py          # Command-line interface
â”œâ”€â”€ tests/              # Test suite
â””â”€â”€ config/             # Configuration files
    â””â”€â”€ llm_config.yaml # LLM provider configurations
```

## Documentation

- **IMPLEMENTATION_GUIDE.md**: Detailed development tasks and implementation notes
- **API Documentation**: Coming soon
- **Configuration Guide**: See `config/llm_config.yaml` for LLM setup examples

---

**Last Updated**: June 1, 2025
**Version**: 0.3.0 (Multi-LLM Connectors & CLI Switching Implemented)
**Status**: ðŸŽ‰ Multi-LLM capabilities and CLI model management complete! Ready for UI development and advanced features.
