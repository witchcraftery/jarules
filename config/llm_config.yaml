# config/llm_config.yaml

# List of available LLM configurations
llm_configs:
  - id: "gemini_flash_default" # Unique identifier for this configuration
    provider: "gemini"        # Maps to the connector class (e.g., GeminiClient)
    description: "Default Google Gemini 1.5 Flash model."
    enabled: true             # Allows disabling a config without removing it
    model_name: "gemini-1.5-flash-latest"
    api_key_env: "GEMINI_API_KEY" # Environment variable that holds the API key
    default_system_prompt: "You are a helpful and concise AI assistant." # Optional
    generation_params:        # Optional: common generation parameters
      temperature: 0.7
      # max_output_tokens: 2048 # Example, Gemini uses specific naming

  - id: "gemini_pro_strict"
    provider: "gemini"
    description: "Google Gemini Pro model with stricter code generation prompt."
    enabled: true
    model_name: "gemini-1.0-pro" # Or relevant pro model
    api_key_env: "GEMINI_API_KEY"
    default_system_prompt: |
      You are an expert AI coding assistant.
      Please provide only the code requested by the user, without any additional explanatory text,
      markdown formatting, or language indicators (like ```python) surrounding the code block.
      If you need to include comments, ensure they are within the code block itself (e.g., using # for Python).
    generation_params:
      temperature: 0.3

  # --- Placeholder Examples (Non-Functional until connectors are built) ---

  - id: "ollama_default_local" # Updated ID for a more generic default
    provider: "ollama"        # Maps to the OllamaConnector
    description: "Default local LLM via Ollama (e.g., Llama 3)."
    enabled: true             # Enable for development
    model_name: "llama3"      # Default model, can be overridden
    api_base_url: "http://localhost:11434" # Standard Ollama local endpoint
    # api_key_env: null # Ollama typically doesn't require an API key for local instances
    default_system_prompt: "You are a helpful AI assistant running on a local Ollama instance."
    generation_params:
      temperature: 0.6
      # Ollama specific parameters can be added here, e.g.:
      # num_ctx: 4096 # Example context window size
      # top_k: 40
      # top_p: 0.9
      # Ensure these match what Ollama API expects if you uncomment them

  - id: "openrouter_default" # Updated ID for a more generic default
    provider: "openrouter"    # Maps to the OpenRouterConnector
    description: "Default OpenRouter configuration (e.g., using a capable free model)."
    enabled: true             # Enable for development
    model_name: "gryphe/mythomax-l2-13b" # A good default free model
    api_key_env_var: "OPENROUTER_API_KEY" # Environment variable for the API key
    api_base_url: "https://openrouter.ai/api/v1" # Default OpenRouter API
    http_referer: "http://localhost:3000" # Example: Update with your actual site URL or app name
    request_timeout: 45 # Seconds
    default_system_prompt: "You are an advanced AI assistant accessed via OpenRouter."
    generation_params:
      temperature: 0.7
      # max_tokens: 1024 # Example, can be overridden
      # top_p: 0.9
      # Add other OpenRouter compatible generation parameters as needed

  - id: "claude_default"
    provider: "claude"        # Maps to the ClaudeConnector
    description: "Default Anthropic Claude configuration (e.g., Claude 3 Haiku)."
    enabled: true
    model_name: "claude-3-haiku-20240307" # Cost-effective and fast model
    api_key_env_var: "ANTHROPIC_API_KEY"  # Environment variable for the API key
    max_tokens: 2048          # Default max tokens for responses
    request_timeout: 60       # Seconds
    anthropic_version_header: "2023-06-01" # Recommended by Anthropic for some features
    default_system_prompt: "You are a helpful and friendly AI assistant powered by Anthropic Claude."
    generation_params:        # Parameters compatible with Anthropic API
      temperature: 0.7
      # top_k: 5
      # top_p: 0.9
      # Other Claude-specific params like "stop_sequences" can be added here

# --- End of Placeholder Examples ---

# General settings for LLM interactions (optional)
# llm_general_settings:
#   default_timeout_seconds: 60
#   retry_attempts: 1
